{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWV6oTijUWra3kgLafiyv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiMl-hub/Gists/blob/main/text_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokeniztaion"
      ],
      "metadata": {
        "id": "mFErODtlnDyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YAjia7tlsoc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download('punkt') # Download the necessary tokenizer models\n",
        "\n",
        "\n",
        "text = \"Hello, world! This is a simple sentence. Tokenization is fun.\"\n",
        "\n",
        "# Word Tokenization\n",
        "words = nltk.word_tokenize(text, language='english')\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Search vs Semantic Search"
      ],
      "metadata": {
        "id": "U1W5iQxQpW-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword Search Example\n",
        "print(\"--- Keyword Search Example ---\")\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A dog barks at the cat.\",\n",
        "    \"Semantic search uses embeddings.\",\n",
        "    \"Keyword search relies on exact word matches.\"\n",
        "]\n",
        "keyword_query = \"dog\"\n",
        "\n",
        "print(f\"Keyword search for: '{keyword_query}'\")\n",
        "keyword_results = []\n",
        "for i, doc in enumerate(documents):\n",
        "    if keyword_query.lower() in doc.lower():\n",
        "        keyword_results.append((i, doc))\n",
        "\n",
        "if keyword_results:\n",
        "    print(\"Found in documents:\")\n",
        "    for i, doc in keyword_results:\n",
        "        print(f\"- Doc {i+1}: {doc}\")\n",
        "else:\n",
        "    print(\"No matching documents found.\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Semantic Search Example\n",
        "print(\"--- Semantic Search Example ---\")\n",
        "\n",
        "# Install sentence-transformers if not already installed\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained model. 'all-MiniLM-L6-v2' is a good general-purpose model.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "corpus = [\n",
        "    \"A cat sits on the mat.\",\n",
        "    \"The dog runs in the park.\",\n",
        "    \"Machine learning is a field of artificial intelligence.\",\n",
        "    \"Natural Language Processing deals with text data.\",\n",
        "    \"This document talks about pets and animals.\",\n",
        "    \"Information retrieval methods include keyword and semantic search.\"\n",
        "]\n",
        "\n",
        "print(\"Corpus documents:\")\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"- {i+1}: {doc}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Encode the corpus to get embeddings\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Define a query for semantic search\n",
        "semantic_query = \"animals in a park\"\n",
        "print(f\"Semantic search query: '{semantic_query}'\")\n",
        "\n",
        "# Encode the query\n",
        "query_embedding = model.encode(semantic_query, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity between query and all corpus embeddings\n",
        "cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "\n",
        "# Combine corpus and scores, then sort by score\n",
        "results = []\n",
        "for i, score in enumerate(cosine_scores):\n",
        "    results.append({'corpus_id': i, 'score': score.item(), 'text': corpus[i]})\n",
        "\n",
        "# Sort the results by score in descending order\n",
        "results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "print(\"\\nTop 3 semantic search results:\")\n",
        "for i, result in enumerate(results[:3]):\n",
        "    print(f\"{i+1}. Score: {result['score']:.4f}, Document: {result['text']}\")"
      ],
      "metadata": {
        "id": "pDWQfgtNl97W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Measuring Vector Distance"
      ],
      "metadata": {
        "id": "4_8j7_tep5ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Define two sample embeddings (vectors)\n",
        "embedding1 = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "embedding2 = np.array([2.0, 3.0, 4.0, 5.0])\n",
        "embedding3 = np.array([-1.0, -2.0, -3.0, -4.0])\n",
        "\n",
        "print(\"Embedding 1:\", embedding1)\n",
        "print(\"Embedding 2:\", embedding2)\n",
        "print(\"Embedding 3:\", embedding3)\n",
        "print(\"\\n-- Similarity between Embedding 1 and Embedding 2 --\")\n",
        "\n",
        "# 1. Euclidean Distance\n",
        "# Lower distance means higher similarity\n",
        "euclidean_dist_1_2 = distance.euclidean(embedding1, embedding2)\n",
        "print(f\"Euclidean Distance: {euclidean_dist_1_2:.4f}\")\n",
        "\n",
        "# 2. Cosine Similarity\n",
        "# Ranges from -1 (opposite) to 1 (identical), 0 (orthogonal)\n",
        "# Using 1 - cosine_distance because scipy's cosine is a distance metric\n",
        "cosine_similarity_1_2 = 1 - distance.cosine(embedding1, embedding2)\n",
        "print(f\"Cosine Similarity: {cosine_similarity_1_2:.4f}\")\n",
        "\n",
        "# 3. Dot Product Similarity\n",
        "# Higher value means higher similarity (especially for non-negative vectors)\n",
        "dot_product_similarity_1_2 = np.dot(embedding1, embedding2)\n",
        "print(f\"Dot Product Similarity: {dot_product_similarity_1_2:.4f}\")\n",
        "\n",
        "print(\"\\n-- Similarity between Embedding 1 and Embedding 3 --\")\n",
        "\n",
        "# Euclidean Distance\n",
        "euclidean_dist_1_3 = distance.euclidean(embedding1, embedding3)\n",
        "print(f\"Euclidean Distance: {euclidean_dist_1_3:.4f}\")\n",
        "\n",
        "# Cosine Similarity\n",
        "cosine_similarity_1_3 = 1 - distance.cosine(embedding1, embedding3)\n",
        "print(f\"Cosine Similarity: {cosine_similarity_1_3:.4f}\")\n",
        "\n",
        "# Dot Product Similarity\n",
        "dot_product_similarity_1_3 = np.dot(embedding1, embedding3)\n",
        "print(f\"Dot Product Similarity: {dot_product_similarity_1_3:.4f}\")"
      ],
      "metadata": {
        "id": "5dmwTddipika"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
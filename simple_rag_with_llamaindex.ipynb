{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiMl-hub/Gists/blob/main/simple_rag_with_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3jr_c46tCeK"
      },
      "source": [
        "# Simple RAG (Retrieval-Augmented Generation) System\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a basic Retrieval-Augmented Generation (RAG) system for processing and querying PDF document(s). The system uses a pipeline that encodes the documents and creates nodes. These nodes then can be used to build a vector index to retrieve relevant information.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text extraction\n",
        "2. Text chunking for manageable processing\n",
        "3. Ingestion pipeline creation using FAISS as vector store and OpenAI embeddings\n",
        "4. Retriever setup for querying the processed documents\n",
        "5. Evaluation of the RAG system\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The PDF is loaded using [SimpleDirectoryReader](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/).\n",
        "2. The text is split into [nodes/chunks](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/) using [SentenceSplitter](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/#sentencesplitter) with specified chunk size and overlap.\n",
        "\n",
        "### Text Cleaning\n",
        "\n",
        "A custom transformation `TextCleaner` is applied to clean the texts. This likely addresses specific formatting issues in the PDF.\n",
        "\n",
        "### Ingestion Pipeline Creation\n",
        "\n",
        "1. OpenAI embeddings are used to create vector representations of the text nodes.\n",
        "2. A FAISS vector store is created from these embeddings for efficient similarity search.\n",
        "\n",
        "### Retriever Setup\n",
        "\n",
        "1. A retriever is configured to fetch the top 2 most relevant chunks for a given query.\n",
        "\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. Modular Design: The ingestion process is encapsulated in a single function for easy reuse.\n",
        "2. Configurable Chunking: Allows adjustment of chunk size and overlap.\n",
        "3. Efficient Retrieval: Uses FAISS for fast similarity search.\n",
        "4. Evaluation: Includes a function to evaluate the RAG system's performance.\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "The code includes a test query: \"What is the main cause of climate change?\". This demonstrates how to use the retriever to fetch relevant context from the processed document.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "The system includes an `evaluate_rag` function to assess the performance of the retriever, though the specific metrics used are not detailed in the provided code.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Scalability: Can handle large documents by processing them in chunks.\n",
        "2. Flexibility: Easy to adjust parameters like chunk size and number of retrieved results.\n",
        "3. Efficiency: Utilizes FAISS for fast similarity search in high-dimensional spaces.\n",
        "4. Integration with Advanced NLP: Uses OpenAI embeddings for state-of-the-art text representation.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This simple RAG system provides a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, it enables efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within large documents or document collections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ho06_I4tCeL"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6BbteXdJtCeL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!pip install faiss-cpu llama-index python-dotenv llama-index-vector-stores-faiss\n",
        "!pip install -U deepeval evaluate langchain-openai langchain langchain-core langchain-community\n",
        "sys.path.append('RAG_TECHNIQUES')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE6m-WWhtCeL"
      },
      "outputs": [],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Te1mVOOtCeM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.schema import BaseNode, TransformComponent\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core.text_splitter import SentenceSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "import faiss\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "\n",
        "EMBED_DIMENSION = 512\n",
        "\n",
        "# llamaindex uses the length of the tokens\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Set embeddig model on LlamaIndex global settings\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46bwq8UatCeM"
      },
      "source": [
        "### Read Docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-CnChkttCeM"
      },
      "outputs": [],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/q_a.json https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/q_a.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip80QUeLtCeM"
      },
      "outputs": [],
      "source": [
        "path = \"data/\"\n",
        "node_parser = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
        "documents = node_parser.load_data()\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHiMuJNtCeN"
      },
      "source": [
        "### Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FaisVectorStore to store embeddings\n",
        "faiss_index = faiss.IndexHNSWFlat(EMBED_DIMENSION, 32) #32 is the number of neighbors in the HNSW graph\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)"
      ],
      "metadata": {
        "id": "QlaAVo1fxpBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYz6i6YgtCeO"
      },
      "source": [
        "### Ingestion Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCkONWsatCeO"
      },
      "outputs": [],
      "source": [
        "text_splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "# Create a pipeline with defined document transformations and vectorstore\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD1ff1SKtCeO"
      },
      "outputs": [],
      "source": [
        "# Run pipeline and get generated nodes from the process\n",
        "nodes = pipeline.run(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes[0]"
      ],
      "metadata": {
        "id": "VhHThiA6WKgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STzo7L6QtCeO"
      },
      "source": [
        "### Create retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK3zDGrutCeO"
      },
      "outputs": [],
      "source": [
        "vector_store_index = VectorStoreIndex(nodes)\n",
        "retriever = vector_store_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMLfezVwtCeO"
      },
      "source": [
        "### Test retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN9AKPqFtCeO"
      },
      "outputs": [],
      "source": [
        "def show_context(context):\n",
        "    \"\"\"\n",
        "    Display the contents of the provided context list.\n",
        "\n",
        "    Args:\n",
        "        context (list): A list of context items to be displayed.\n",
        "\n",
        "    Prints each context item in the list with a heading indicating its position.\n",
        "    \"\"\"\n",
        "    for i, c in enumerate(context):\n",
        "        print(f\"Context {i+1}:\")\n",
        "        print(c.text)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFfdM80stCeO"
      },
      "outputs": [],
      "source": [
        "test_query = \"What is the main cause of climate change?\"\n",
        "context = retriever.retrieve(test_query)\n",
        "show_context(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ekvt5AtCeP"
      },
      "source": [
        "### Let's see how well does it perform using \"LLM-as-a-Judge\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mh42aoatCeP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "\n",
        "# Set llm model for evaluation of the question and answers\n",
        "LLM_MODEL = \"gpt-5-nano\"\n",
        "\n",
        "\n",
        "# Define evaluation metrics\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    model=LLM_MODEL,\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
        "    ],\n",
        "    evaluation_steps=[\n",
        "        \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "faithfulness_metric = FaithfulnessMetric(\n",
        "    threshold=0.7,\n",
        "    model=LLM_MODEL,\n",
        "    include_reason=False\n",
        ")\n",
        "\n",
        "relevance_metric = ContextualRelevancyMetric(\n",
        "    threshold=0.7,\n",
        "    model=LLM_MODEL,\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "def evaluate_rag(query_engine, num_questions: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Evaluate the RAG system using predefined metrics.\n",
        "\n",
        "    Args:\n",
        "        query_engine: Query engine to ask questions and get answers along with retrieved context.\n",
        "        num_questions (int): Number of questions to evaluate (default: 5).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load questions and answers from JSON file\n",
        "    q_a_file_name = \"data/q_a.json\"\n",
        "    with open(q_a_file_name, \"r\", encoding=\"utf-8\") as json_file:\n",
        "        q_a = json.load(json_file)\n",
        "\n",
        "    questions = [qa[\"question\"] for qa in q_a][:num_questions]\n",
        "    ground_truth_answers = [qa[\"answer\"] for qa in q_a][:num_questions]\n",
        "    generated_answers = []\n",
        "    retrieved_contexts = []\n",
        "\n",
        "    # Generate answers and retrieve documents for each question\n",
        "    for question in questions:\n",
        "        response = query_engine.query(question)\n",
        "        context = [node.text for node in response.source_nodes] if hasattr(response, 'source_nodes') else []\n",
        "        retrieved_contexts.append(context)\n",
        "        generated_answers.append(str(response) if hasattr(response, '__str__') else str(response))\n",
        "\n",
        "    # Create test cases and evaluate\n",
        "    test_cases = []\n",
        "    for i in range(len(questions)):\n",
        "        test_case = LLMTestCase(\n",
        "            input=questions[i],\n",
        "            actual_output=generated_answers[i],\n",
        "            expected_output=ground_truth_answers[i],\n",
        "            retrieval_context=retrieved_contexts[i]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "\n",
        "    evaluate(\n",
        "        test_cases=test_cases,\n",
        "        metrics=[correctness_metric, faithfulness_metric, relevance_metric]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-UOVO5LtCeP"
      },
      "source": [
        "### Evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOSaC_kJtCeP"
      },
      "outputs": [],
      "source": [
        "query_engine  = vector_store_index.as_query_engine(similarity_top_k=2)\n",
        "evaluate_rag(query_engine, num_questions=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mXE7Zy-tCeP"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--simple-rag-with-llamaindex)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}